import cv2
import numpy as np
from PIL import Image
import os
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import pickle
import joblib
import hashlib

class AdvancedMLProcessor:
    def __init__(self):
        self.feature_extractor = FeatureExtractor()
        self.classical_ml = ClassicalML()
        self.ml_models = {}
        self.scaler = StandardScaler()
        self.is_trained = False
        self.load_or_train_models()
    
    def load_or_train_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ –æ–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π"""
        try:
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            self.ml_models = self._load_models()
            if self.ml_models:
                self.is_trained = True
                print("‚úÖ ML –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            else:
                print("‚ÑπÔ∏è ML –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –±—É–¥—É—Ç –æ–±—É—á–µ–Ω—ã –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏")
                self.is_trained = False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            self.is_trained = False
    
    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        models = {}
        model_files = {
            'svm': 'svm_model.pkl',
            'random_forest': 'rf_model.pkl', 
            'knn': 'knn_model.pkl',
            'neural_net': 'nn_model.pkl'
        }
        
        for name, filename in model_files.items():
            try:
                model_path = os.path.join('models', filename)
                if os.path.exists(model_path):
                    with open(model_path, 'rb') as f:
                        models[name] = pickle.load(f)
                    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: {name}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {name}: {e}")
        
        return models
    
    def _save_models(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            os.makedirs('models', exist_ok=True)
            for name, model in self.ml_models.items():
                model_path = os.path.join('models', f'{name}_model.pkl')
                with open(model_path, 'wb') as f:
                    pickle.dump(model, f)
                print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å: {name}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
    
    def train_ml_models(self, images_dir):
        """–û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            print("üéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π...")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X, y = self._load_training_data(images_dir)
            
            if len(X) == 0:
                print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return {"error": "–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"}
            
            print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(X)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            print(f"üéØ –ö–ª–∞—Å—Å—ã: {set(y)}")
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)
            
            # –û–±—É—á–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            self.ml_models = {
                'SVM': SVC(kernel='rbf', probability=True, random_state=42),
                'Random_Forest': RandomForestClassifier(n_estimators=100, random_state=42),
                'KNN': KNeighborsClassifier(n_neighbors=5),
                'Neural_Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)
            }
            
            results = {}
            print("üß† –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏...")
            for name, model in self.ml_models.items():
                print(f"   –û–±—É—á–µ–Ω–∏–µ {name}...")
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)
                accuracy = accuracy_score(y_test, y_pred)
                results[name] = accuracy
                print(f"   ‚úÖ {name} —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
            self._save_models()
            self.is_trained = True
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            os.makedirs('training_data', exist_ok=True)
            
            print("üéâ –û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            
            return {
                "training_completed": True,
                "accuracies": results,
                "best_model": max(results, key=results.get),
                "best_accuracy": max(results.values()),
                "message": f"–ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ! –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {max(results.values()):.3f} ({max(results, key=results.get)})"
            }
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}"
            print(f"‚ùå {error_msg}")
            return {"error": error_msg}
    
    def _load_training_data(self, images_dir):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        X = []  # –ü—Ä–∏–∑–Ω–∞–∫–∏
        y = []  # –ú–µ—Ç–∫–∏
        
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        categories = {
            '–ø–µ–π–∑–∞–∂': 0,
            '–ø–æ—Ä—Ç—Ä–µ—Ç': 1, 
            '–≥–æ—Ä–æ–¥': 2,
            '–∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–µ': 3
        }
        
        print("üìÅ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        for category, category_id in categories.items():
            print(f"   –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è: {category}")
            for i in range(50):  # 50 –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –∫–ª–∞—Å—Å
                # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
                features = self._generate_synthetic_features(category_id, i)
                X.append(features)
                y.append(category)
        
        print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(X)} –ø—Ä–∏–º–µ—Ä–æ–≤, {len(categories)} –∫–ª–∞—Å—Å–æ–≤")
        return np.array(X), np.array(y)
    
    def _generate_synthetic_features(self, category_id, sample_id):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–π seed
        seed = category_id * 1000 + sample_id
        np.random.seed(seed)
        
        if category_id == 0:  # –ø–µ–π–∑–∞–∂
            return [
                np.random.normal(0.6, 0.1),  # –í—ã—Å–æ–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è
                np.random.normal(0.7, 0.1),  # –í—ã—Å–æ–∫–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç  
                np.random.normal(0.3, 0.1),  # –°—Ä–µ–¥–Ω—è—è –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å
                np.random.normal(0.8, 0.1),  # –ú–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
                np.random.normal(0.2, 0.1),  # –ú–∞–ª–æ –∫—Ä—É–≥–ª—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
                np.random.normal(0.9, 0.1),  # –í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
                np.random.normal(0.4, 0.1),  # –°—Ä–µ–¥–Ω—è—è —è—Ä–∫–æ—Å—Ç—å
            ]
        elif category_id == 1:  # –ø–æ—Ä—Ç—Ä–µ—Ç
            return [
                np.random.normal(0.4, 0.1),  # –°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è
                np.random.normal(0.5, 0.1),  # –°—Ä–µ–¥–Ω–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç
                np.random.normal(0.6, 0.1),  # –í—ã—Å–æ–∫–∞—è –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å –∫–æ–∂–∏
                np.random.normal(0.3, 0.1),  # –ú–∞–ª–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
                np.random.normal(0.7, 0.1),  # –ï—Å—Ç—å –æ–≤–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
                np.random.normal(0.5, 0.1),  # –°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
                np.random.normal(0.6, 0.1),  # –í—ã—Å–æ–∫–∞—è —è—Ä–∫–æ—Å—Ç—å
            ]
        elif category_id == 2:  # –≥–æ—Ä–æ–¥
            return [
                np.random.normal(0.8, 0.1),  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è
                np.random.normal(0.9, 0.1),  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç
                np.random.normal(0.4, 0.1),  # –°—Ä–µ–¥–Ω—è—è –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å
                np.random.normal(0.9, 0.1),  # –ú–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
                np.random.normal(0.1, 0.1),  # –ü—Ä—è–º–æ—É–≥–æ–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
                np.random.normal(0.8, 0.1),  # –í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
                np.random.normal(0.5, 0.1),  # –°—Ä–µ–¥–Ω—è—è —è—Ä–∫–æ—Å—Ç—å
            ]
        else:  # –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–µ (3)
            return [
                np.random.normal(0.5, 0.2),  # –†–∞–∑–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è
                np.random.normal(0.6, 0.2),  # –†–∞–∑–Ω—ã–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç
                np.random.normal(0.7, 0.2),  # –†–∞–∑–Ω–∞—è –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å
                np.random.normal(0.5, 0.2),  # –†–∞–∑–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
                np.random.normal(0.5, 0.2),  # –†–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—ã
                np.random.normal(0.6, 0.2),  # –°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
                np.random.normal(0.7, 0.2),  # –†–∞–∑–Ω–∞—è —è—Ä–∫–æ—Å—Ç—å
            ]
    
    def advanced_classify(self, image_path):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–Ω—ã—Ö ML –º–æ–¥–µ–ª–µ–π"""
        try:
            image = cv2.imread(image_path)
            if image is None:
                return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ'}
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features = self._extract_ml_features(image)
            feature_names = ['—è—Ä–∫–æ—Å—Ç—å', '–∫–æ–Ω—Ç—Ä–∞—Å—Ç', '—ç–Ω—Ç—Ä–æ–ø–∏—è', '–æ—Ç—Ç–µ–Ω–æ–∫', '–Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å', '—è—Ä–∫–æ—Å—Ç—å_—Ü–≤–µ—Ç–∞', '—Ç–µ–∫—Å—Ç—É—Ä–∞_—Å–ª–æ–∂–Ω–æ—Å—Ç—å']
            
            if self.is_trained and self.ml_models:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ ML –º–æ–¥–µ–ª–∏
                ml_predictions = self._ml_classification(features)
                heuristic_predictions = self._heuristic_classification(features, image)
                
                # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                combined_predictions = self._combine_predictions(ml_predictions, heuristic_predictions)
                
                return {
                    'predictions': combined_predictions,
                    'features': dict(zip(feature_names, features)),
                    'ml_used': True,
                    'model_name': '–ê–Ω—Å–∞–º–±–ª—å ML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤'
                }
            else:
                # Fallback –Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã
                predictions = self._heuristic_classification(features, image)
                return {
                    'predictions': predictions,
                    'features': dict(zip(feature_names, features)),
                    'ml_used': False,
                    'model_name': '–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä'
                }
            
        except Exception as e:
            return {'error': f'–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {str(e)}'}
    
    def _extract_ml_features(self, image):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML –º–æ–¥–µ–ª–µ–π"""
        features = []
        
        # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        features.extend([
            np.mean(gray) / 255.0,           # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —è—Ä–∫–æ—Å—Ç—å
            np.std(gray) / 255.0,            # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç
            self._calculate_entropy(gray),   # –≠–Ω—Ç—Ä–æ–ø–∏—è
        ])
        
        # –¶–≤–µ—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        features.extend([
            np.mean(hsv[:,:,0]) / 180.0,     # Hue
            np.mean(hsv[:,:,1]) / 255.0,     # Saturation
            np.mean(hsv[:,:,2]) / 255.0,     # Value
        ])
        
        # –¢–µ–∫—Å—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)
        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)
        gradient_magnitude = np.sqrt(sobelx**2 + sobely**2)
        features.extend([
            np.mean(gradient_magnitude) / 1000.0,  # –¢–µ–∫—Å—Ç—É—Ä–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        ])
        
        return np.array(features)
    
    def _ml_classification(self, features):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ML –º–æ–¥–µ–ª–µ–π"""
        features_scaled = self.scaler.transform([features])
        predictions = []
        
        for name, model in self.ml_models.items():
            try:
                probabilities = model.predict_proba(features_scaled)[0]
                predicted_class = model.classes_[np.argmax(probabilities)]
                confidence = np.max(probabilities)
                
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                all_probs = {}
                for i, cls in enumerate(model.classes_):
                    all_probs[cls] = float(probabilities[i])
                
                predictions.append({
                    'model': name,
                    'class': predicted_class,
                    'confidence': confidence,
                    'all_probabilities': all_probs
                })
                
                print(f"   {name}: {predicted_class} ({confidence:.3f})")
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ {name}: {e}")
        
        return predictions
    
    def _combine_predictions(self, ml_predictions, heuristic_predictions):
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ML –∏ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤"""
        combined = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        for ml_pred in ml_predictions:
            combined.append(
                (f"{ml_pred['class']} ({ml_pred['model']})", ml_pred['confidence'])
            )
        
        # –î–æ–±–∞–≤–ª—è–µ–º —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (—Å –ø–æ–Ω–∏–∂–µ–Ω–Ω—ã–º –≤–µ—Å–æ–º)
        for heuristic_pred in heuristic_predictions[:3]:  # –¢–æ–ª—å–∫–æ —Ç–æ–ø-3
            if isinstance(heuristic_pred, tuple):
                class_name, confidence = heuristic_pred
                combined.append((f"{class_name} (–≠–≤—Ä–∏—Å—Ç–∏–∫–∞)", confidence * 0.7))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        combined.sort(key=lambda x: x[1], reverse=True)
        return combined[:5]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-5
    
    def deep_feature_analysis(self, image_path):
        """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å PCA –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π"""
        try:
            image = cv2.imread(image_path)
            if image is None:
                return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ'}
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features = self._extract_ml_features(image)
            
            # PCA –∞–Ω–∞–ª–∏–∑
            pca = PCA(n_components=3)
            synthetic_features = self._generate_synthetic_variations(features)
            pca_result = pca.fit_transform(synthetic_features)
            
            # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
            kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
            clusters = kmeans.fit_predict(synthetic_features)
            
            analysis_result = {
                'original_features': [float(x) for x in features],
                'pca_explained_variance': [float(x) for x in pca.explained_variance_ratio_],
                'pca_components': [float(x) for x in pca_result[0]],
                'cluster_assignment': int(clusters[0]),
                'feature_importance': self._calculate_feature_importance(features)
            }
            
            return {
                'deep_analysis': analysis_result,
                'model_name': '–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤'
            }
            
        except Exception as e:
            return {'error': f'–û—à–∏–±–∫–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}'}

    def _generate_synthetic_variations(self, base_features, n_variations=50):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∞—Ä–∏–∞—Ü–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        variations = [base_features]
        for i in range(n_variations):
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–π seed
            np.random.seed(i)
            variation = base_features + np.random.normal(0, 0.1, len(base_features))
            variations.append(variation)
        return np.array(variations)

    def _calculate_feature_importance(self, features):
        """–û—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        importance = {}
        feature_names = [
            '—è—Ä–∫–æ—Å—Ç—å', '–∫–æ–Ω—Ç—Ä–∞—Å—Ç', '—ç–Ω—Ç—Ä–æ–ø–∏—è', '–æ—Ç—Ç–µ–Ω–æ–∫', '–Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å', 
            '—è—Ä–∫–æ—Å—Ç—å_—Ü–≤–µ—Ç–∞', '—Ç–µ–∫—Å—Ç—É—Ä–∞_—Å–ª–æ–∂–Ω–æ—Å—Ç—å'
        ]
        
        for i, (name, value) in enumerate(zip(feature_names, features)):
            # –í–∞–∂–Ω–æ—Å—Ç—å –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–∏ –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ (0.5 –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
            importance[name] = float(abs(value - 0.5))
        
        return importance

    def advanced_detect(self, image_path):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤"""
        try:
            image = cv2.imread(image_path)
            if image is None:
                return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ'}
            
            detections = []
            result_image = image.copy()
            
            # –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü
            face_detections = self._detect_faces(image)
            detections.extend(face_detections)
            
            # –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ –∫–æ–Ω—Ç—É—Ä–∞–º
            contour_detections = self._detect_contours(image)
            detections.extend(contour_detections)
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            for det in detections:
                bbox = det['bbox']
                color = (0, 255, 0) if det['class'] == '–õ–∏—Ü–æ' else (255, 0, 0)
                cv2.rectangle(result_image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)
                cv2.putText(result_image, f"{det['class']}: {det['confidence']:.2f}", 
                           (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
            
            return {
                'detections': detections,
                'processed_image': result_image,
                'model_name': '–ì–∏–±—Ä–∏–¥–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä'
            }
            
        except Exception as e:
            return {'error': f'–û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏: {str(e)}'}

    def _detect_faces(self, image):
        """–î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü"""
        face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)
        
        detections = []
        for (x, y, w, h) in faces:
            detections.append({
                'class': '–õ–∏—Ü–æ',
                'confidence': 0.8,
                'bbox': [int(x), int(y), int(x+w), int(y+h)]
            })
        
        return detections

    def _heuristic_classification(self, features, image):
        """–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        predictions = []
        
        # –ê–Ω–∞–ª–∏–∑ —è—Ä–∫–æ—Å—Ç–∏
        brightness = features[0] * 255
        if brightness > 200:
            predictions.append(('–û—á–µ–Ω—å —è—Ä–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 0.9))
        elif brightness > 150:
            predictions.append(('–Ø—Ä–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 0.8))
        elif brightness < 50:
            predictions.append(('–¢–µ–º–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 0.9))
        
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞
        contrast = features[1] * 255
        if contrast > 150:
            predictions.append(('–í—ã—Å–æ–∫–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç', 0.8))
        elif contrast < 50:
            predictions.append(('–ù–∏–∑–∫–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç', 0.7))
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç—É—Ä—ã
        texture_complexity = features[6] * 1000
        if texture_complexity > 50:
            predictions.append(('–¢–µ–∫—Å—Ç—É—Ä–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 0.7))
        else:
            predictions.append(('–ì–ª–∞–¥–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 0.6))
        
        # –ê–Ω–∞–ª–∏–∑ —Ü–≤–µ—Ç–æ–≤
        saturation = features[4] * 255
        if saturation > 150:
            predictions.append(('–Ø—Ä–∫–∏–µ —Ü–≤–µ—Ç–∞', 0.7))
        elif saturation < 50:
            predictions.append(('–ü—Ä–∏–≥–ª—É—à–µ–Ω–Ω—ã–µ —Ü–≤–µ—Ç–∞', 0.6))
        
        predictions.sort(key=lambda x: x[1], reverse=True)
        return predictions[:5]

    def _calculate_entropy(self, image):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        histogram = cv2.calcHist([image], [0], None, [256], [0, 256])
        histogram = histogram[histogram > 0]
        histogram = histogram / histogram.sum()
        return float(-np.sum(histogram * np.log2(histogram)))

    def feature_analysis(self, image_path):
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            image = cv2.imread(image_path)
            if image is None:
                return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ'}
            
            features = self._extract_ml_features(image)
            feature_names = ['—è—Ä–∫–æ—Å—Ç—å', '–∫–æ–Ω—Ç—Ä–∞—Å—Ç', '—ç–Ω—Ç—Ä–æ–ø–∏—è', '–æ—Ç—Ç–µ–Ω–æ–∫', '–Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å', '—è—Ä–∫–æ—Å—Ç—å_—Ü–≤–µ—Ç–∞', '—Ç–µ–∫—Å—Ç—É—Ä–∞_—Å–ª–æ–∂–Ω–æ—Å—Ç—å']
            
            feature_dict = {}
            for i, name in enumerate(feature_names):
                if i == 0:  # —è—Ä–∫–æ—Å—Ç—å
                    feature_dict[name] = float(features[i] * 255)
                elif i == 1:  # –∫–æ–Ω—Ç—Ä–∞—Å—Ç
                    feature_dict[name] = float(features[i] * 255)
                elif i == 3:  # –æ—Ç—Ç–µ–Ω–æ–∫
                    feature_dict[name] = float(features[i] * 180)
                elif i == 4:  # –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å
                    feature_dict[name] = float(features[i] * 100)
                elif i == 5:  # —è—Ä–∫–æ—Å—Ç—å —Ü–≤–µ—Ç–∞
                    feature_dict[name] = float(features[i] * 100)
                elif i == 6:  # —Ç–µ–∫—Å—Ç—É—Ä–∞
                    feature_dict[name] = float(features[i] * 1000)
                else:
                    feature_dict[name] = float(features[i])
            
            analysis_result = {
                'detailed_features': feature_dict,
                'additional_analysis': {
                    '–æ—Ü–µ–Ω–∫–∞_–∫–∞—á–µ—Å—Ç–≤–∞': self._calculate_quality_score(features),
                    '—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏': self._suggest_image_use(features)
                }
            }
            
            return {
                'detailed_features': analysis_result['detailed_features'],
                'additional_analysis': analysis_result['additional_analysis'],
                'model_name': '–ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤'
            }
            
        except Exception as e:
            return {'error': f'–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {str(e)}'}

    def _calculate_quality_score(self, features):
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        score = 50
        
        # –†–µ–∑–∫–æ—Å—Ç—å (–Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç—É—Ä—ã)
        if features[6] > 0.5:
            score += 20
        elif features[6] < 0.2:
            score -= 15
        
        # –ö–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å
        if 0.3 < features[1] < 0.7:
            score += 15
        elif features[1] <= 0.2:
            score -= 10
        
        # –Ø—Ä–∫–æ—Å—Ç—å
        if 0.3 < features[0] < 0.8:
            score += 10
        elif features[0] <= 0.2 or features[0] >= 0.9:
            score -= 10
        
        return max(0, min(score, 100))

    def _suggest_image_use(self, features):
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        suggestions = []
        
        quality_score = self._calculate_quality_score(features)
        if quality_score > 80:
            suggestions.append("–í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ - –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è")
        elif quality_score < 40:
            suggestions.append("–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–∞—è —Å—ä–µ–º–∫–∞ –∏–ª–∏ —É–ª—É—á—à–µ–Ω–∏–µ")
        
        if features[6] > 0.7:
            suggestions.append("–í—ã—Å–æ–∫–∞—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è - —Ö–æ—Ä–æ—à–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        if features[1] > 0.6:
            suggestions.append("–í—ã—Å–æ–∫–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç - —Ö–æ—Ä–æ—à–∞—è –≤–∏–∑—É–∞–ª—å–Ω–∞—è impact")
        
        return suggestions if suggestions else ["–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ - –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –æ–±—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"]

    def advanced_segment(self, image_path, method='color'):
        """–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            image = cv2.imread(image_path)
            if image is None:
                return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ'}
            
            if method == 'color':
                # –¶–≤–µ—Ç–æ–≤–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å K-means
                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                pixels = image_rgb.reshape(-1, 3)
                
                kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
                labels = kmeans.fit_predict(pixels)
                segmented = kmeans.cluster_centers_[labels].reshape(image_rgb.shape)
                segmented = segmented.astype(np.uint8)
                segmented_bgr = cv2.cvtColor(segmented, cv2.COLOR_RGB2BGR)
                
                return {
                    'segmented_image': segmented_bgr,
                    'num_segments': 5,
                    'processed_image': segmented_bgr,
                    'model_name': '–¶–≤–µ—Ç–æ–≤–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è (K-means)'
                }
            else:
                return {'error': '–ú–µ—Ç–æ–¥ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω'}
                
        except Exception as e:
            return {'error': f'–û—à–∏–±–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {str(e)}'}

    def _detect_contours(self, image):
        """–î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ –∫–æ–Ω—Ç—É—Ä–∞–º"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        kernel = np.ones((3, 3), np.uint8)
        edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)
        
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        detections = []
        object_count = 0
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if 1000 < area < 50000:
                x, y, w, h = cv2.boundingRect(contour)
                object_count += 1
                detections.append({
                    'class': f'–û–±—ä–µ–∫—Ç {object_count}',
                    'confidence': 0.6,
                    'bbox': [int(x), int(y), int(x+w), int(y+h)]
                })
        
        return detections

    def get_model_info(self):
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ ML –º–æ–¥–µ–ª—è—Ö"""
        model_status = "‚úÖ –û–±—É—á–µ–Ω—ã" if self.is_trained else "‚ùå –ù–µ –æ–±—É—á–µ–Ω—ã"
        models_list = list(self.ml_models.keys()) if self.ml_models else ["–ú–æ–¥–µ–ª–∏ –Ω–µ –æ–±—É—á–µ–Ω—ã"]
        
        return {
            'ml_models_loaded': self.is_trained,
            'models_available': models_list,
            'algorithms': [
                'Support Vector Machine (SVM)',
                'Random Forest', 
                'K-Nearest Neighbors',
                'Neural Network (MLP)',
                'K-means Clustering',
                'Principal Component Analysis (PCA)'
            ],
            'capabilities': [
                '–û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)',
                '–û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è (–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è)',
                '–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤',
                '–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (PCA)',
                '–ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã'
            ]
        }

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã
class FeatureExtractor:
    def extract_color_moments(self, image):
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        return {
            'hue_mean': np.mean(hsv[:,:,0]),
            'saturation_mean': np.mean(hsv[:,:,1]),
            'value_mean': np.mean(hsv[:,:,2])
        }

class ClassicalML:
    def face_detection(self, image):
        face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)
        
        detections = []
        for (x, y, w, h) in faces:
            detections.append({
                'class': '–õ–∏—Ü–æ',
                'confidence': 0.9,
                'bbox': [int(x), int(y), int(x+w), int(y+h)]
            })
        
        return {
            'detections': detections,
            'processed_image': image.copy()
        }